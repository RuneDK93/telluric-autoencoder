{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for preprocessing the solar observations to make them ready for input to the autoencoder network.\n",
    "\n",
    "The notebook performs the following preprocessing steps. \n",
    " 1. Load Data\n",
    " 2. Connect observations with correct blaze and wave files\n",
    " 3. Blaze correct spectra\n",
    " 4. Filter out low flux and very high airmass observations \n",
    " 5. Interpolate all observations to common wavelength grid \n",
    " 6. Take natural logarithm and continuum normalise all observations\n",
    "\n",
    "The notebook is set up to perform preprocessing of observations in the format of newer HARPS-N 2D solar observations. If one whises to train on solar spectra from another spectrograph this notebook will not work. You can either modify this notebook to the data format of the spectrograph or carry out your own preprocessing. The important part is to obtain blaze corrected continuum normalised log spectra interpolated to the same wavelength axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data was loaded in 54.041800022125244 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from astropy.io import fits\n",
    "import h5py\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "import math\n",
    "from spectrum_overload import Spectrum\n",
    "start = time.time()\n",
    "print(\"Loading data...\")\n",
    "\n",
    "# Initializing \n",
    "data_matrix=[]\n",
    "blaze_matrix=[]\n",
    "wave_matrix=[]\n",
    "airmass = []\n",
    "berv = []\n",
    "file_name = []\n",
    "wave_name = []\n",
    "blaze_name = []\n",
    "wave_to_use = []\n",
    "blaze_to_use = []\n",
    "\n",
    "for path, subdirs, files in os.walk(\"../Solar_Spectra/data\"):\n",
    "    for name in files:       \n",
    "        # Loading Blaze files\n",
    "        if name.endswith(\"blaze_A.fits\"):\n",
    "            fn = os.path.join(path, name) #filename\n",
    "            blaze = fits.getdata(fn)\n",
    "            blaze_matrix.append(blaze)\n",
    "            blaze_name.append(fn)            \n",
    "\n",
    "        # Loading Wave files  \n",
    "        if name.endswith(\"wave_A.fits\"):\n",
    "            fn = os.path.join(path, name) #filename\n",
    "            wave = fits.getdata(fn)            \n",
    "            wave_matrix.append(wave)\n",
    "            wave_name.append(fn)\n",
    "            \n",
    "        # Loading Spectrum files\n",
    "        if name.endswith(\"e2ds_A.fits\"):\n",
    "            fn       = os.path.join(path, name) #filename\n",
    "            tmp_data = fits.getdata(fn)       #read data entry \n",
    "            header = fits.getheader(fn)      # read header\n",
    "            \n",
    "        # Select only solar observations\n",
    "            if header['PROGRAM'] == 'SOLAR':\n",
    "                data_matrix.append(tmp_data)\n",
    "                airmass.append(header['AIRMASS'])\n",
    "                berv.append(header['HIERARCH TNG DRS BERV'])\n",
    "                wave_to_use.append(header['HIERARCH TNG DRS CAL TH FILE'])\n",
    "                blaze_to_use.append(header['HIERARCH TNG DRS BLAZE FILE'])\n",
    "                file_name.append(fn)\n",
    "\n",
    "print(\"Data was loaded in\", time.time()-start, \"seconds\")\n",
    "spectrum = np.asarray(data_matrix)\n",
    "Airmass = np.asarray(airmass)\n",
    "Berv = np.asarray(berv)\n",
    "blaze = np.asarray(blaze_matrix)\n",
    "wave = np.asarray(wave_matrix)\n",
    "blaze_name = np.asarray(blaze_name)\n",
    "wave_name = np.asarray(wave_name)\n",
    "blaze_to_use = np.asarray(blaze_to_use)\n",
    "wave_to_use = np.asarray(wave_to_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting spectra to their respective wave and blaze files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting spetra to their respective wave and blaze files\n",
    "correct_wave = np.empty_like(spectrum)\n",
    "for i in range(len(wave_to_use)):\n",
    "    for j in range(len(wave_name)):\n",
    "        if wave_to_use[i][:-12] == wave_name[j][33:-12]: # Compare only the relevant part of the file names\n",
    "            correct_wave[i] = wave[j]                    # The region to compare depends on directory name length\n",
    "\n",
    "correct_blaze = np.empty_like(spectrum)\n",
    "for i in range(len(blaze_to_use)):\n",
    "    for j in range(len(blaze_name)):\n",
    "        if blaze_to_use[i][:-12] == blaze_name[j][33:-12]: # Compare only the relevant part of the file names\n",
    "            correct_blaze[i] = blaze[j]                    # The region to compare depends on directory name length\n",
    "\n",
    "# Blaze correcting spectra\n",
    "corrected_spectrum = spectrum / correct_blaze\n",
    "\n",
    "#Combining blaze corrected flux and wavelength into data array\n",
    "Data = np.array([corrected_spectrum,correct_wave])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing blaze files\n",
      "No missing wave files\n"
     ]
    }
   ],
   "source": [
    "# Checking if all wave and blaze files were found\n",
    "missing = []\n",
    "for i in range(len(correct_blaze)):\n",
    "    if correct_blaze[i,0,0]==0:\n",
    "        missing.append(i)\n",
    "        \n",
    "missing = np.array([]).astype(int)\n",
    "missing = np.asarray(missing)  \n",
    "if len(missing)==0:\n",
    "    print('No missing blaze files')\n",
    "if len(missing)>0:\n",
    "    print(len(missing),'Missing blaze file')\n",
    "    print(blaze_to_use[missing])\n",
    "    print('For observation')\n",
    "    print(missing)\n",
    "\n",
    "missing = []\n",
    "for i in range(len(correct_wave)):\n",
    "    if correct_wave[i,0,0]==0:\n",
    "        missing.append(i)\n",
    "        \n",
    "missing = np.array([]).astype(int)\n",
    "missing = np.asarray(missing)   \n",
    "if len(missing)==0:\n",
    "    print('No missing wave files')\n",
    "if len(missing)>0:\n",
    "    print(len(missing),'Missing wave files')\n",
    "    print(wave_to_use[missing])\n",
    "    print('For observation')\n",
    "    print(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed spectra for high airmass: 239\n",
      "Number of removed spectra for low flux: 180\n",
      "Data shape\n",
      "(838, 69, 2, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Filtering away observations with missing wave / blaze files:\n",
    "if len(missing)>0:\n",
    "    Data = np.delete(Data, missing ,axis=1)\n",
    "    Airmass = np.delete(Airmass,missing,axis=0)\n",
    "    Berv = np.delete(Berv,missing,axis=0)\n",
    "    print('Number of removed spectra for missing blaze or wave files',len(missing))\n",
    "\n",
    "# Filtering away observation with airmass > 2.0\n",
    "index1 = []\n",
    "for i in range(len(Airmass)):\n",
    "    if  Airmass[i]  > 2: \n",
    "        index1.append(i)          \n",
    "\n",
    "# Filtering the observations from index out \n",
    "if len(index1)>0:\n",
    "    Data = np.delete(Data, index1 ,axis=1)\n",
    "    Airmass = np.delete(Airmass,index1,axis=0)\n",
    "    Berv = np.delete(Berv,index1,axis=0)\n",
    "    print('Number of removed spectra for high airmass:',len(index1))\n",
    "\n",
    "# Finding mean flux of spectra\n",
    "means = np.zeros(len(Data[0]))\n",
    "for i in range(len(Data[0])):\n",
    "    means[i] = np.mean(Data[0,i,:])\n",
    "    \n",
    "#Some of the observations have very low flux values. \n",
    "# Finding index of low flux observations\n",
    "index2 = []\n",
    "for i in range(len(Data[0])):\n",
    "    if  np.max(means) / np.mean(Data[0,i,:])  > 1.2: \n",
    "        index2.append(i)       \n",
    "\n",
    "# Filtering the low flux observations from index out \n",
    "if len(index2)>0:\n",
    "    filtered_Data = np.delete(Data, index2 ,axis=1)\n",
    "    Airmass = np.delete(Airmass,index2,axis=0)\n",
    "    Berv = np.delete(Berv,index2,axis=0)\n",
    "    print('Number of removed spectra for low flux:',len(index2)) \n",
    "\n",
    "D = filtered_Data.transpose(1,2,0,3) # Transposing data array to have observations as first index\n",
    "\n",
    "# For the specific training data we are using \n",
    "# orders 0,1,2,6 and 25 have negative flux values in their spectrum. \n",
    "# These are converted to a flux value of 1 for stability when taking the natural logarithm, \n",
    "D[:,0,0,:] = np.where(D[:,0,0,:]<1, 1, D[:,0,0,:])\n",
    "D[:,1,0,:] = np.where(D[:,1,0,:]<1, 1, D[:,1,0,:])\n",
    "D[:,2,0,:] = np.where(D[:,2,0,:]<1, 1, D[:,2,0,:])\n",
    "D[:,6,0,:] = np.where(D[:,6,0,:]<1, 1, D[:,6,0,:])\n",
    "D[:,25,0,:] = np.where(D[:,25,0,:]<1, 1, D[:,25,0,:])\n",
    "\n",
    "# Finding mean flux of spectra and saving for later scaling \n",
    "means = np.zeros(len(D))\n",
    "for i in range(len(D)):\n",
    "    means[i] = np.mean(D[i,:])\n",
    "const = np.log(np.mean(means))\n",
    "\n",
    "print('Data shape')\n",
    "print(D.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolating to common wavelength axis and continuum normalizing spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "K = D.shape[1]       # number of apertures to combine (here 69)\n",
    "resolution = D.shape[3] # number of pixels (here 4096)\n",
    "\n",
    "combined_wave = np.zeros((K,resolution))\n",
    "combined_flux = np.zeros((K,D.shape[0],resolution))\n",
    "for k in range(K):\n",
    "\n",
    "    # Interpolating to common restframe grid of wavepoints \n",
    "    aperture = k\n",
    "\n",
    "    # Finding min and max for interpolation. \n",
    "    # This is needed as the observations do not cover the exact same wavelength regions\n",
    "    MIN =  math.ceil(np.min(D[:,aperture,1,0]))   # Wave min\n",
    "    MAX =  math.floor(np.max(D[:,aperture,1,-1])) # Wave max\n",
    "\n",
    "    common_wave = np.linspace(MIN, MAX, num=resolution) \n",
    "    interpol_flux = []\n",
    "\n",
    "    for i in range(len(D)):\n",
    "        flux = D[i,aperture,0,:]  \n",
    "        wave = D[i,aperture,1,:]\n",
    "        f = interpolate.interp1d(wave, flux)\n",
    "\n",
    "        int_flux = f(common_wave)   # use interpolation function returned by `interp1d`\n",
    "        interpol_flux.append(int_flux)\n",
    "    interpol_flux = np.array(interpol_flux)\n",
    "    val_save = interpol_flux   \n",
    "    \n",
    "    # Continuum normalisation\n",
    "    # Can be performed with your own choice of continuum normalization procedure\n",
    "    # Here using spectrum_overload package\n",
    "    interpol_flux=np.log(interpol_flux) # taking log of spectrum \n",
    "    normalized_flux = np.zeros((len(interpol_flux),len(interpol_flux[0])))\n",
    "    for j in range(len(interpol_flux)):\n",
    "        s = Spectrum(flux=interpol_flux[j], xaxis=common_wave)\n",
    "        continuum = s.continuum(method=\"linear\", nbins=10, ntop=5) # Optimal continuum normalisaton params can depend on spectrum size\n",
    "        normalized_flux[j] = interpol_flux[j]/continuum.flux\n",
    "    interpol_flux=normalized_flux\n",
    "    \n",
    "    # Combining the wave function and flux of each aperture\n",
    "    combined_wave[k] = common_wave\n",
    "    combined_flux[k] = interpol_flux\n",
    "    \n",
    "Preproc_wave=combined_wave\n",
    "Preproc_flux=combined_flux.transpose(1,0,2)\n",
    "Preproc_airmass = Airmass\n",
    "Preproc_berv = Berv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Preproc Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file\n",
    "#output = open('../preproc/preproc_wave.pkl', 'wb')\n",
    "#pickle.dump(preproc_wave, output)\n",
    "#output.close()\n",
    "\n",
    "#output = open('../preproc/preproc_flux.pkl', 'wb')\n",
    "#pickle.dump(Preproc_flux, output)\n",
    "#output.close()\n",
    "\n",
    "#output = open('../preproc/preproc_airmass.pkl', 'wb')\n",
    "#pickle.dump(preproc_airmass, output)\n",
    "#output.close()\n",
    "\n",
    "#output = open('../preproc/preproc_berv.pkl', 'wb')\n",
    "#pickle.dump(preproc_berv, output)\n",
    "#output.close()\n",
    "\n",
    "#output = open('../preproc/preproc_const.pkl', 'wb')\n",
    "#pickle.dump(const, output)\n",
    "#output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
